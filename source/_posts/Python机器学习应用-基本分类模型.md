---
title: Python机器学习应用 | 基本分类模型
date: 2017-06-23 13:33:26
categories: [机器学习]
---
## 1 K近邻分类器（KNN）

KNN：通过计算待分类数据点与已有数据集中的所有数据点的距离，取距离最小的前K个点，根据“少数服从多数”的原则，将这个数据点划分为出现次数最多的那个类别。
![](http://oltfslql1.bkt.clouddn.com/knn.jpg)

### 1.1 sklearn中的K近邻分类器

在sklearn库中，可以使用sklearn.neighbors.KNeighborsClassifier创建一个K近邻分类器，主要参数有：
（1）n_neighbors：用于指定分类器中K的大小（默认值为5，注意与kmeans的区别）
（2）weights：设置选中的K个点对分类结果影响的权重（默认值为平均权重“uniform”，可以选择“distance”代码越近的点权重越高，或者传入自己编写的以距离为参数的权重计算函数）
（3）algorithm：设置用于计算临近点的方法，因为数据量很大的情况下计算当前点和所有点的距离再选出最近的k各点，这个计算量是很费时的，所以（选项中有ball_tree、kd_tree和brute，分别代表不同的寻找邻居的优化算法，默认值为auto，根据训练数据自动选择）

### 1.2 K近邻分类器的使用

创建一组数据X和它对应的标签y：
```
>>> X=[[0],[1],[2],[3]]
>>> y=[0,0,1,1]
```
使用import语句导入K近邻分类器
```
>>> from sklearn.neighbors import KNeighborsClassifier
```
参数n_neighbors设置为3，即使用最近的3个邻居作为分类的依据，其他参数保持默认值，并将创建好的实例赋给变量neigh。
```
>>> neigh = KNeighborsClassifier(n_neighbors=3)
```
调用fit()函数，将训练数据X和标签y送入分类器进行学习。
```
>>> neigh.fit(X,y)
```
调用predict()函数，对未知分类样本[1.1]分类，可以直接并将需要分类的数据构造为数组形式作为参数传入，得到分类标签作为返回值。
```
>>> print(neigh.predict([[1.1]]))
0
```
样例输出值为0，表示K近邻分类器通过计算样本[1.1]与训练数据的距离取0,1,2这3个邻居作为依据，根据“投票法”最终将样本分为类别0。

### 1.3 KNN的使用经验

在实际使用时，我们可以使用所有训练结构构成特征X和标签y，使用fit()函数进行训练。在正式分类时，通过一次性构造测试集或者一个一个输入样本的方式，得到样本对应的分类结果。有关K的取值：
（1）如果K较大，相当于使用较大领域中的训练实例进行预测，可以减小估计误差，但是距离较远的样本也会对预测起作用，导致预测错误。
（2）如果K较小，相当于使用较小的领域进行预测，如果邻居恰好是噪声点，会导致过拟合。
（3）一般情况下，K会倾向选取较小的值，并使用交叉验证法选取最优K值。

## 2 决策树

决策树是一种树形结构的分类器，通过顺序询问分类点的属性决定分类点最终的类别。通常根据特征的信息增益或其他指标，构建一颗决策树。在分类时，只需要按照决策树中的结点依次进行判断，即可得到样本所属类别。

### 2.1 sklearn中的决策树

在sklearn库中，可以使用sklearn.tree.DecisionTreeClassifier创建一个决策树用于分类，其主要参数有：
（1）criterion：用于选择属性的准则，可以传入“个i你”代表基尼系数，或者“entropy”代表信息增益。
（2）max_features：表示在决策树结点进行分裂时，从多少个特征中选择最优特征。可以设定固定数目、百分比或其他标准。它的默认值是使用所有特征个数。

### 2.2 决策树的使用

首先，我们导入sklearn内嵌的鸢尾花数据集：
```
>>> from sklearn.datasets import load_iris
```
接下来，我们使用import语句导入决策树分类器，同时导入计算交叉验证值的函数cross_avl_score。
```
>>> from sklearn.tree import DecisionTreeClassifier
>>> from sklearn.model_selection import cross_val_score
```
我们使用默认参数，创建一颗基于基尼系数的决策树，并将该决策树分类器赋值给变量clf。
```
>>> clf = DecisionTreeClassifier()
```
将鸢尾花数据赋值给变量iris
```
>>> iris = load_iris()
```
这里我们将决策树分类器做为待评估的模型，iris.data鸢尾花数据做为特征，iris.target鸢尾花分类标签做为目标结果，通过设定cv为10，使用10折交叉验证。得到最终的交叉验证得分。
```
>>> cross_val_score(clf,iris.data,iris.target,cv=10)
array([ 1.,  0.93333333,  1.,  0.93333333,  0.93333333,  0.86666667,  0.93333333,  0.93333333,  1.,  1.])
```
以仿照之前K近邻分类器的使用方法，利用fit()函数训练模型并使用predict()函数预测：
```
>>> clf.fit(X,y)
>>> clf.predict(x)
```

### 2.3 决策树使用经验

决策树本质上是寻找一种对特征空间上的划分，旨在构建一个训练数据拟合的好，并且复杂度小的决策树。

在实际使用中，需要根据数据情况，调整DecisionTreeClassifier类中传入的参数，比如选择合适的criterion，设置随机变量等。

## 3 朴素贝叶斯

朴素贝叶斯分类器是一个以贝叶斯定理为基础的多分类的分类器。
对于给定数据，首先基于特征的条件独立性假设，学习输入输出的联合概率分布，然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出y。
$$p(A|B) = \frac{p(B|A)*p(A)}{p(B)}$$

### 3.1 sklearn中的朴素贝叶斯

在sklearn库中，实现了三个朴素贝叶斯分类器，如下：
| 分类器 | 描述 |
| :--- | :--- |
| naive_bayes.GussianNB | 高斯朴素贝叶斯 |
| naive_bayes.MultinamialNB | 针对多项式模型的朴素贝叶斯分类器 |
| naive_bayes.BernoulliNB | 针对多元伯努利模型的朴素贝叶斯分类器 |

区别在于假设某一特征的所有属于某个类别的观测值符合特定分布。

### 3.2 sklearn中的朴素贝叶斯

在sklearn库中，可以使用sklearn.naive_bayes.GaussianNB创建一个高斯朴素贝叶斯分类器，其参数有：
- priors：给定各个类别的先验概率。如果为空，则按训练数据的实际情况进行统计；如果给定先验概率，则在训练过程中不能更改。

### 3.3 朴素贝叶斯的使用

导入numpy库，并构造训练数据X和y
```
>>> import numpy as np
>>> X = np.array([[-1,-1],[-2,-1],[-3,-2],[1,1],[2,1],[3,2]])
>>> Y = np.array([1,1,1,2,2,2])
```
使用import语句导入朴素贝叶斯分类器
```
>>> from sklearn.naive_bayes import GaussianNB
```
使用默认参数，创建一个高斯朴素贝叶斯分类器，并将该分类器赋值给clf。
```
>>> clf = GaussianNB(priors=None)
```
类似的，使用fit()函数进行训练，并使用predict()函数进行预测，得到预测结果为1。（测试时可以构造二维数组达到同时预测多个样本的目的）
```
>>> clf.fit(X,Y)
>>> print(clf.predict([[-0.8,-1]]))
[1]
```

### 3.3 朴素贝叶斯使用经验

朴素贝叶斯是典型的生成学习方法，由训练数据学习联合概率分布，并求得后验概率分布。

朴素贝叶斯一般在小规模数据上的表现非常好，适合进行多分类任务。